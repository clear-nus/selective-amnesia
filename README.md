<div align="center">

# Selective Amnesia: A Continual Learning Approach for Forgetting in Deep Generative Models

[![preprint](https://img.shields.io/static/v1?label=arXiv&message=2301.10120&color=B31B1B)](https://arxiv.org/abs/2305.10120)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Venue:NeurIPS 2023](https://img.shields.io/badge/Venue-NeurIPS%202023-007CFF)](https://nips.cc/)

</div>

<p align="center">
  <img src="./assets/main_fig.png" width="70%">
  <br />
  <span>Figure 1: Qualitative results of our method, Selective Amnesia (SA). SA can be applied to a variety
of models, from forgetting textual prompts such as specific celebrities or nudity in text-to-image
models to discrete classes in VAEs and diffusion models (DDPM).</span>
</p>


This is the official code repository for the NeurIPS 2023 Spotlight paper [Selective Amnesia: A Continual Learning Approach for Forgetting in Deep Generative Models](https://arxiv.org/abs/2305.10120).

The code is split into three subfolders, one each for VAE, DDPM and Stable Diffusion experiments. Detailed instructions are included in the respective subfolders.

## Contact
If you have any questions regarding the code or the paper, please email [Alvin](mailto:alvinh@comp.nus.edu.sg).

## BibTeX
If you find this repository or the ideas presented in our paper useful, please consider citing.
```
@article{heng2023selective,
  title={Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models},
  author={Heng, Alvin and Soh, Harold},
  journal={arXiv preprint arXiv:2305.10120},
  year={2023}
}
```
